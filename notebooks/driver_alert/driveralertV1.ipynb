{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f751dd-a25a-4fff-a0b5-b6cf8ad51999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'drowsiness-detection'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 8 (delta 1), reused 4 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rizkydermawan1992/drowsiness-detection.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e255a2-8fdc-4a03-857e-11e1f7dce87a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff962f10-ad86-4394-b657-2a3f93752258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jetson/Notebooks/driveralert/drowsiness-detection\n"
     ]
    }
   ],
   "source": [
    "%cd drowsiness-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042a984e-09fd-4e65-a782-bf4471bf3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_landmark_detection.py  README.md\n",
      "main.py                     shape_predictor_68_face_landmarks.dat\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8618e523-c442-4304-bdb3-cd283a0455d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils import face_utils\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "import pyfirmata\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "import time\n",
    "from jetcam.usb_camera import USBCamera\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import traitlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d7284d-6593-4a66-bef0-3c2de0fd7e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c422b6e06c4ea691590a1b7d36cab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = USBCamera(capture_device=0)\n",
    "#camera.read()\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='Dormido', min=0.0, max=1.0, orientation='vertical')\n",
    "speed_slider = widgets.FloatSlider(description='Somnoliento', min=0.0, max=0.5, value=0.0, step=0.01, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.VBox([widgets.HBox([image_widget, blocked_slider, speed_slider]) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8471355-4835-40e7-aa97-47c86efb1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n",
      "[INFO] starting video stream thread...\n"
     ]
    }
   ],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "\t# compute the euclidean distances between the two sets of\n",
    "\t# vertical eye landmarks (x, y)-coordinates\n",
    "\tA = dist.euclidean(eye[1], eye[5])\n",
    "\tB = dist.euclidean(eye[2], eye[4])\n",
    "\n",
    "\t# compute the euclidean distance between the horizontal\n",
    "\t# eye landmark (x, y)-coordinates\n",
    "\tC = dist.euclidean(eye[0], eye[3])\n",
    "\n",
    "\t# compute the eye aspect ratio\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\n",
    "\t# return the eye aspect ratio\n",
    "\treturn ear\n",
    "\n",
    "def mouth_aspect_ratio(mouth):\n",
    "\t\n",
    "\tA = dist.euclidean(mouth[13], mouth[19])\n",
    "\tB = dist.euclidean(mouth[14], mouth[18])\n",
    "\tC = dist.euclidean(mouth[15], mouth[17])\n",
    "\n",
    "\t# compute the mouth aspect ratio\n",
    "\tmar = (A+B+C)/3.0\n",
    "\n",
    "\t# return the eye aspect ratio\n",
    "\treturn mar\n",
    "\n",
    "# define two constants, one for the eye aspect ratio to indicate\n",
    "# blink and then a second constant for the number of consecutive\n",
    "# frames the eye must be below the threshold for to set off the\n",
    "# alarm\n",
    "EYE_AR_THRESH = 0.25\n",
    "MOUTH_AR_THRESH = 35\n",
    "EYE_AR_CONSEC_FRAMES = 10\n",
    "\n",
    "# initialize the frame counter as well as a boolean used to\n",
    "# indicate if the alarm is going off\n",
    "COUNTER = 0\n",
    "ALARM_ON = False\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# grab the indexes of the facial landmarks for the left and\n",
    "# right eye, respectively\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "\n",
    "# start the video stream thread\n",
    "print(\"[INFO] starting video stream thread...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3113baa-e741-4945-b2d3-053aa4a29c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(change):\n",
    "    global COUNTER\n",
    "    image = change['new']\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  \n",
    "    rects = detector(image, 0)\n",
    "      \n",
    "    # loop over the face detections\n",
    "    for rect in rects:\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shapes = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shapes)\n",
    "\n",
    "        # extract the left and right eye coordinates, then use the\n",
    "        # coordinates to compute the eye aspect ratio for both eyes\n",
    "        mouth    = shape[mStart:mEnd]\n",
    "        leftEye  = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        mar      = mouth_aspect_ratio(mouth)\n",
    "        leftEAR  = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        # average the eye aspect ratio together for both eyes\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "\n",
    "        # compute the convex hull for the left and right eye, then\n",
    "        # visualize each of the eyes\n",
    "        mouthHull    = cv2.convexHull(mouth)\n",
    "        leftEyeHull  = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(image, [mouthHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(image, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(image, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "\n",
    "\n",
    "        # check to see if the eye aspect ratio is below the blink\n",
    "        # threshold, and if so, increment the blink frame counter\n",
    "        if mar > MOUTH_AR_THRESH or ear < EYE_AR_THRESH :\n",
    "            COUNTER += 1\n",
    "\n",
    "            # if the eyes were closed for a sufficient number of\n",
    "            # then sound the alarm\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:               \n",
    "                blocked_slider.value = 1\n",
    "        # otherwise, the eye aspect ratio is not below the blink\n",
    "        # threshold, so reset the counter and alarm\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            blocked_slider.value = 0\n",
    "\n",
    "        # draw the computed eye aspect ratio on the frame to help\n",
    "        # with debugging and setting the correct eye aspect ratio\n",
    "        # thresholds and frame counters\n",
    "\n",
    "\n",
    "    #cv2.putText(image, \"EAR: {:.2f}\".format(ear), (900, 70),\n",
    "    #cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "   # cv2.putText(image, \"MAR: {:.2f}\".format(mar), (900, 110),\n",
    "   # cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "    time.sleep(0.001)\n",
    "        \n",
    "update({'new': camera.value})  # we call the function once to initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d4b6e9-6817-48c5-928e-f6cd169b8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.running = True\n",
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de3f616e-ab10-4074-9ebc-8c4ca25658f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4d95e-abe2-44df-b003-7b9b2ea23951",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
